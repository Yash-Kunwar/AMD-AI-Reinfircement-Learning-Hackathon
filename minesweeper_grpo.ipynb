{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper LLM Competition - Custom GRPO Training\n",
    "\n",
    "## Goal\n",
    "Finetune an LLM with LoRA using GRPO to play Minesweeper by:\n",
    "- **Input**: JSON game state (board configuration)\n",
    "- **Output**: JSON action (reveal or flag a cell)\n",
    "\n",
    "Teams will compete to train the best Minesweeper-playing LLM!\n",
    "\n",
    "## Training Approach\n",
    "- **Model**: GPT-OSS 20B with LoRA or other models in the /root/.cache/huggingface/hub directory [**Any model other than /root/.cache/huggingface/hub will lead to disqualification**]\n",
    "- **Method**: GRPO (Group Relative Policy Optimization), SFT or any RL-policies (not just strict to use GRPO)\n",
    "- **Framework**: Unsloth (2-6x faster, 70% less VRAM)\n",
    "- **Hardware**: AMD GPU (ROCm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model with Unsloth\n",
    "\n",
    "Load GPT-OSS 20B with LoRA configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--Qwen--Qwen3-4B\n"
     ]
    }
   ],
   "source": [
    "!ls -1 /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 02-15 09:03:33 [__init__.py:225] Automatically detected platform rocm.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.6: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.1rc2.dev161+g8a297115e.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+gitb2fb688. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[2026-02-15 09:03:36] INFO modeling.py:1004: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3ba79537e0474fb80c431d55d9af57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.makedirs('/workspace/huggingface_cache', exist_ok=True)\n",
    "\n",
    "max_seq_length = 1024  # Max context length\n",
    "lora_rank = 64    # LoRA rank (higher = smarter but slower; 4 is too low for reasoning tasks)\n",
    "\n",
    "model_path = \"/root/.cache/huggingface/hub/models--Qwen--Qwen3-4B/snapshots/1cfa9a7208912126459214e8b04321603b3df60c/\"\n",
    "\n",
    "# Try loading with explicit torch_dtype\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = max_seq_length,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# Force model to cuda explicitly\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LoRA Adapters\n",
    "\n",
    "Add LoRA layers for efficient finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.6 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank * 2,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper Game Implementation\n",
    "\n",
    "Custom Minesweeper environment supporting:\n",
    "- Customizable board size and mine count\n",
    "- Actions: reveal or flag cells\n",
    "- Win: reveal all safe cells\n",
    "- Lose: reveal a mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional, Set\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class MinesweeperGame:\n",
    "    rows: int\n",
    "    cols: int\n",
    "    num_mines: int\n",
    "    seed: Optional[int] = None\n",
    "    _rng: random.Random = field(init=False, repr=False)\n",
    "    _board: List[List[int]] = field(init=False, repr=False)  # -1 = mine, 0-8 = count\n",
    "    _revealed: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _flagged: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _state: str = field(default=\"ongoing\", init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.num_mines >= self.rows * self.cols:\n",
    "            raise ValueError(\"Too many mines for board size\")\n",
    "        self._rng = random.Random(self.seed)\n",
    "        self._board = [[0 for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        self._place_mines()\n",
    "        self._calculate_numbers()\n",
    "\n",
    "    def _place_mines(self):\n",
    "        \"\"\"Place mines randomly on the board\"\"\"\n",
    "        positions = [(r, c) for r in range(self.rows) for c in range(self.cols)]\n",
    "        mine_positions = self._rng.sample(positions, self.num_mines)\n",
    "        for r, c in mine_positions:\n",
    "            self._board[r][c] = -1\n",
    "\n",
    "    def _calculate_numbers(self):\n",
    "        \"\"\"Calculate numbers for each cell based on adjacent mines\"\"\"\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self._board[r][c] == -1:\n",
    "                    continue\n",
    "                count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if 0 <= nr < self.rows and 0 <= nc < self.cols:\n",
    "                            if self._board[nr][nc] == -1:\n",
    "                                count += 1\n",
    "                self._board[r][c] = count\n",
    "\n",
    "    def _reveal_cell(self, row: int, col: int) -> bool:\n",
    "        \"\"\"Reveal a cell. Returns True if valid move, False if invalid.\n",
    "        Uses iterative flood-fill to avoid recursion limit on large boards.\n",
    "        (Issue #11: was recursive; Issue typo: fixed 'bself' -> 'self')\n",
    "        \"\"\"\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            return False\n",
    "        if (row, col) in self._revealed or (row, col) in self._flagged:\n",
    "            return False\n",
    "\n",
    "        stack = [(row, col)]\n",
    "        while stack:\n",
    "            r, c = stack.pop()\n",
    "            if (r, c) in self._revealed:\n",
    "                continue\n",
    "\n",
    "            self._revealed.add((r, c))\n",
    "\n",
    "            # Hit a mine!\n",
    "            if self._board[r][c] == -1:\n",
    "                self._state = \"failed\"\n",
    "                return True\n",
    "\n",
    "            # Auto-reveal neighbors if cell is 0\n",
    "            if self._board[r][c] == 0:\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if (0 <= nr < self.rows and 0 <= nc < self.cols\n",
    "                                and (nr, nc) not in self._revealed\n",
    "                                and (nr, nc) not in self._flagged):\n",
    "                            stack.append((nr, nc))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _flag_cell(self, row: int, col: int) -> bool:\n",
    "        \"\"\"Flag/unflag a cell. Returns True if valid, False if invalid\"\"\"\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            return False\n",
    "        if (row, col) in self._revealed:\n",
    "            return False\n",
    "        \n",
    "        if (row, col) in self._flagged:\n",
    "            self._flagged.remove((row, col))\n",
    "        else:\n",
    "            self._flagged.add((row, col))\n",
    "        return True\n",
    "\n",
    "    def do_action(self, action: dict) -> str:\n",
    "        \"\"\"Execute an action and return a status string.\n",
    "\n",
    "        Returns one of:\n",
    "          'ok'               - valid move executed\n",
    "          'mine'             - revealed a mine (game over)\n",
    "          'win'              - game won after this move\n",
    "          'invalid_format'   - bad action dict / missing keys / bad types\n",
    "          'out_of_bounds'    - coordinates outside the board\n",
    "          'already_revealed' - cell was already revealed\n",
    "          'flagged_cell'     - tried to reveal a flagged cell\n",
    "          'invalid_flag'     - tried to flag a revealed cell\n",
    "          'game_over'        - game was already over before this call\n",
    "\n",
    "        (Issue #13: previously set state='failed' for ALL invalid moves,\n",
    "         conflating formatting errors with hitting a mine.)\n",
    "        \"\"\"\n",
    "        if self._state != \"ongoing\":\n",
    "            return \"game_over\"\n",
    "\n",
    "        if not isinstance(action, dict):\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        action_type = action.get(\"type\")\n",
    "        row = action.get(\"row\")\n",
    "        col = action.get(\"col\")\n",
    "\n",
    "        if action_type not in [\"reveal\", \"flag\"] or row is None or col is None:\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        try:\n",
    "            row, col = int(row), int(col)\n",
    "        except (ValueError, TypeError):\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            self._state = \"failed\"\n",
    "            return \"out_of_bounds\"\n",
    "\n",
    "        if action_type == \"reveal\":\n",
    "            if (row, col) in self._revealed:\n",
    "                self._state = \"failed\"\n",
    "                return \"already_revealed\"\n",
    "            if (row, col) in self._flagged:\n",
    "                self._state = \"failed\"\n",
    "                return \"flagged_cell\"\n",
    "            valid = self._reveal_cell(row, col)\n",
    "        else:\n",
    "            if (row, col) in self._revealed:\n",
    "                self._state = \"failed\"\n",
    "                return \"invalid_flag\"\n",
    "            valid = self._flag_cell(row, col)\n",
    "\n",
    "        if not valid:\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        self._check_win()\n",
    "\n",
    "        if self._state == \"failed\":\n",
    "            return \"mine\"\n",
    "        if self._state == \"success\":\n",
    "            return \"win\"\n",
    "        return \"ok\"\n",
    "\n",
    "    def _check_win(self):\n",
    "        \"\"\"Check if player has won\"\"\"\n",
    "        total_cells = self.rows * self.cols\n",
    "        safe_cells = total_cells - self.num_mines\n",
    "        if len(self._revealed) == safe_cells:\n",
    "            self._state = \"success\"\n",
    "\n",
    "    def get_visible_board(self) -> List[List[str]]:\n",
    "        \"\"\"Get board state as player sees it\"\"\"\n",
    "        visible = []\n",
    "        for r in range(self.rows):\n",
    "            row = []\n",
    "            for c in range(self.cols):\n",
    "                if (r, c) in self._flagged:\n",
    "                    row.append('F')\n",
    "                elif (r, c) in self._revealed:\n",
    "                    val = self._board[r][c]\n",
    "                    row.append('*' if val == -1 else str(val))\n",
    "                else:\n",
    "                    row.append('.')\n",
    "            visible.append(row)\n",
    "        return visible\n",
    "\n",
    "    def state(self) -> str:\n",
    "        return self._state\n",
    "\n",
    "    def pretty_print(self) -> str:\n",
    "        \"\"\"Pretty print the board\"\"\"\n",
    "        visible = self.get_visible_board()\n",
    "        lines = []\n",
    "        \n",
    "        # Header\n",
    "        header = \"   \" + \" \".join(f\"{i:2d}\" for i in range(self.cols))\n",
    "        lines.append(header)\n",
    "        lines.append(\"  \" + \"â”€\" * (self.cols * 3 + 1))\n",
    "        \n",
    "        # Board\n",
    "        for r, row in enumerate(visible):\n",
    "            line = f\"{r:2d}â”‚ \" + \"  \".join(row)\n",
    "            lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  1  2  3  4  5\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      " 0â”‚ .  .  .  .  .  .\n",
      " 1â”‚ .  .  .  .  .  .\n",
      " 2â”‚ .  .  .  .  .  .\n",
      " 3â”‚ .  .  .  .  .  .\n",
      " 4â”‚ .  .  .  .  .  .\n",
      " 5â”‚ .  .  .  .  .  .\n",
      "State: ongoing\n",
      "\n",
      "After revealing (0,0):\n",
      "    0  1  2  3  4  5\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      " 0â”‚ 0  1  .  .  .  .\n",
      " 1â”‚ 0  1  .  .  .  .\n",
      " 2â”‚ 0  1  1  1  1  .\n",
      " 3â”‚ 0  0  0  0  1  1\n",
      " 4â”‚ 0  0  0  0  0  0\n",
      " 5â”‚ 0  0  0  0  0  0\n",
      "State: ongoing\n"
     ]
    }
   ],
   "source": [
    "# Create test game\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "print(game.pretty_print())\n",
    "print(f\"State: {game.state()}\")\n",
    "\n",
    "# Test action\n",
    "game.do_action({\"type\": \"reveal\", \"row\": 0, \"col\": 0})\n",
    "print(\"\\nAfter revealing (0,0):\")\n",
    "print(game.pretty_print())\n",
    "print(f\"State: {game.state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Input/Output Format\n",
    "\n",
    "## Input Format (Game State)\n",
    "```json\n",
    "{\n",
    "  \"board\": [\n",
    "    [\"1\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"]\n",
    "  ],\n",
    "  \"rows\": 6,\n",
    "  \"cols\": 6,\n",
    "  \"mines\": 5,\n",
    "  \"flags_placed\": 0,\n",
    "  \"cells_revealed\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "## Output Format (Action)\n",
    "```json\n",
    "{\"type\": \"reveal\", \"row\": 2, \"col\": 3}\n",
    "```\n",
    "or\n",
    "```json\n",
    "{\"type\": \"flag\", \"row\": 1, \"col\": 4}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Minesweeper AI. Analyze the state and make a move.\n",
      "\n",
      "CRITICAL RULES:\n",
      "- DO NOT use <think> tags.\n",
      "- DO NOT output any reasoning, explanations, or text.\n",
      "- Start your response IMMEDIATELY with { and end with }.\n",
      "- ONLY output a valid JSON object.\n",
      "\n",
      "Game state:\n",
      "{\n",
      "  \"board_grid\": \". . . . . .\\n. . . . . .\\n. . . . . .\\n. . . . . .\\n. . . . . .\\n. . . . . .\",\n",
      "  \"rows\": 6,\n",
      "  \"cols\": 6,\n",
      "  \"mines\": 5,\n",
      "  \"flags_placed\": 0,\n",
      "  \"cells_revealed\": 0\n",
      "}\n",
      "\n",
      "Legend:\n",
      "- \".\" = unrevealed\n",
      "- \"F\" = flagged\n",
      "- \"0\"-\"8\"...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "Important Hints:\n",
    "\n",
    "1. Prompt is crucial - make sure your LLM is not verbose and do not write/output reasoning, instead the verbose must be hidden or abstracted and\n",
    "    output must be JSON object - the verbosity in our experiment led to running out of max tokens set and\n",
    "    thus JSON parsing failure - i.e. Disqualification:\n",
    "    {{\"type\": \"reveal\", \"row\": <row_index>, \"col\": <col_index>}}\n",
    "    or\n",
    "    {{\"type\": \"flag\", \"row\": <row_index>, \"col\": <col_index>}}\n",
    "\n",
    "2. Make sure your model learns generic N*M game board shapes and # number of mines\n",
    "\n",
    "3. Do not flag the cell which is already flagged - game will go in recursion and you will have heavy penalty\n",
    "\n",
    "4. Do not flag the cell which is already revealed - game will go in recursion and you will have heavy penalty\n",
    "'''\n",
    "\n",
    "def format_state_for_llm(game: MinesweeperGame) -> str:\n",
    "    \"\"\"Convert game state to a highly compressed JSON prompt for LLM\"\"\"\n",
    "    \n",
    "    # 1. Compress the 2D array into a tight string grid to save hundreds of tokens\n",
    "    board_str = \"\\n\".join([\" \".join(row) for row in game.get_visible_board()])\n",
    "    \n",
    "    state = {\n",
    "        \"board_grid\": board_str, \n",
    "        \"rows\": game.rows,\n",
    "        \"cols\": game.cols,\n",
    "        \"mines\": game.num_mines,\n",
    "        \"flags_placed\": len(game._flagged),\n",
    "        \"cells_revealed\": len(game._revealed),\n",
    "    }\n",
    "\n",
    "    # 2. Aggressive Anti-Verbosity Prompt\n",
    "    prompt = f\"\"\"You are a Minesweeper AI. Analyze the state and make a move.\n",
    "    \n",
    "CRITICAL RULES:\n",
    "- DO NOT use <think> tags.\n",
    "- DO NOT output any reasoning, explanations, or text.\n",
    "- Start your response IMMEDIATELY with {{ and end with }}.\n",
    "- ONLY output a valid JSON object.\n",
    "\n",
    "Game state:\n",
    "{json.dumps(state, indent=2)}\n",
    "\n",
    "Legend:\n",
    "- \".\" = unrevealed\n",
    "- \"F\" = flagged\n",
    "- \"0\"-\"8\" = adjacent mines\n",
    "\n",
    "Format:\n",
    "{{\"type\": \"reveal\", \"row\": <R>, \"col\": <C>}}\n",
    "or\n",
    "{{\"type\": \"flag\", \"row\": <R>, \"col\": <C>}}\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def parse_llm_action(response: str) -> dict:\n",
    "    \"\"\"Extract JSON action from LLM response.\n",
    "    \n",
    "    Finds all JSON-like objects and returns the LAST one matching the\n",
    "    expected schema.  LLMs typically reason through options and place\n",
    "    their final answer at the end, so taking the last valid match is\n",
    "    more robust than taking the first.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    best = None\n",
    "    for match in re.finditer(r'\\{[^{}]*\\}', response):\n",
    "        try:\n",
    "            action = json.loads(match.group())\n",
    "            if (\"type\" in action and \"row\" in action and \"col\" in action\n",
    "                    and action[\"type\"] in [\"reveal\", \"flag\"]):\n",
    "                best = action\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "# Test formatting\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "prompt = format_state_for_llm(game)\n",
    "print(prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model Before Training\n",
    "\n",
    "See how the base model performs without finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base Model Response ===\n",
      "<think>\n",
      "Okay, let's see. The user provided a Minesweeper game state where the board is entirely unrevealed, with 6x6 grid, 5 mines, and no flags or revealed cells. My job is to figure out the best move as an AI.\n",
      "\n",
      "First, since the board is all unrevealed, there's no immediate information to deduce mine locations. But with 5 mines in 36 cells, the probability of each cell being a mine is 5/36, which is about 14%. But since the AI needs to make a move, maybe the best approach is to start by revealing a cell that's in a position where it can give more information. However, since there are no flags or revealed cells, the AI can't use any existing clues.\n",
      "\n",
      "But wait, the rules say that the AI must make a move. Since there are no flags placed yet, maybe the AI should start by revealing a cell. But how to choose which one? Since there's no information, maybe the safest bet is to pick a cell that's in the middle, but that's just a guess. Alternatively, maybe the AI should flag cells based on some probability, but the problem is that the AI can't flag unless it's certain. But in this case, since there are no revealed cells, the AI can't determine any adjacent mines. \n",
      "\n",
      "Wait, the rules say that the AI must output either a reveal or a flag. But since there are no flags placed, and no revealed cells, maybe the AI should start by revealing a cell. However, the AI might not know where the mines are. But since the game is in a state where all cells are unrevealed, the AI can't make any flags yet. So the first move would be to reveal a cell. But which one?\n",
      "\n",
      "In Minesweeper, usually, the first move is to click a cell that's not a mine. Since there are 5 mines, the probability of the first click being a mine is 5/36, which is about 14%. But the AI can't know that. However, the AI is supposed to make a move. Since there's no information, the AI has to choose a cell to reveal. Maybe the best approach is to pick a cell that's in the middle, like (3,3) or something. But the problem is that the AI has to choose a specific cell. Since the board is all dots, the AI can't know anything.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=42)\n",
    "prompt = format_state_for_llm(game)\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    ")\n",
    "\n",
    "print(\"=== Base Model Response ===\")\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.3, # Lowered temperature to discourage creative rambling\n",
    "    max_new_tokens = 512,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Reward Functions\n",
    "\n",
    "Define reward functions to guide the model's learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_json_reward(completions, **kwargs):\n",
    "    \"\"\"Reward valid JSON action format and heavily penalize <think> tags.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"]\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        score = 0.0\n",
    "        \n",
    "        # 1. Massive penalty for verbosity or <think> tags\n",
    "        if \"<think>\" in response or response.strip()[:1] != \"{\":\n",
    "            score -= 10.0  # Punish it heavily for not starting immediately with JSON\n",
    "            \n",
    "        # 2. Reward or punish based on parsing success\n",
    "        if action is None:\n",
    "            score -= 50.0  # -50 as per the AMD rules for Invalid JSON\n",
    "        else:\n",
    "            score += 5.0   # Base reward for successfully formatting a JSON\n",
    "            \n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def gameplay_scores(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Scoring Criteria:\n",
    "    1.  Flag cell that IS a mine        â†’ +15\n",
    "    2.  Flag cell that is NOT a mine    â†’ -10\n",
    "    3.  Reveal cell that IS a mine      â†’ -25 (round over, team goes to next round)\n",
    "    4.  Reveal cell that is safe        â†’ +10 or +15 (+10 is for randomly guessed OR +15 if logically deducible)\n",
    "    5.  Flag already flagged cell       â†’ -12\n",
    "    6.  Reveal already revealed cell    â†’ -12\n",
    "    7.  Out of bounds                   â†’ -15\n",
    "    8.  Total flags > total mines       â†’ -10\n",
    "    9.  Invalid JSON                    â†’ -50\n",
    "    10. Win the game                    â†’ +100 (big bonus) - Winning here means Flagging all the mines + Revealing all the safe cells\n",
    "    11. Reveal a flagged cell           â†’ -8\n",
    "    12. Flag a revealed cell            â†’ -8\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    # Get game state info passed from dataset\n",
    "    seeds = kwargs.get(\"seed\", [])\n",
    "    move_histories = kwargs.get(\"move_history\", [])\n",
    "\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        # Criterion 12: Invalid JSON\n",
    "        if action is None:\n",
    "            scores.append(-50.0)\n",
    "            continue\n",
    "\n",
    "        # Reconstruct EXACT game state\n",
    "        if idx < len(seeds) and idx < len(move_histories):\n",
    "            seed = seeds[idx]\n",
    "            move_history_raw = move_histories[idx]\n",
    "\n",
    "            if isinstance(move_history_raw, str):\n",
    "                move_history = json.loads(move_history_raw)\n",
    "            else:\n",
    "                move_history = move_history_raw\n",
    "\n",
    "            # Reconstruct game\n",
    "            # Note: kwargs wraps dataset columns in lists during GRPO batched processing\n",
    "            r = kwargs.get(\"rows\", [4] * len(completions))[idx]\n",
    "            c = kwargs.get(\"cols\", [4] * len(completions))[idx]\n",
    "            m = kwargs.get(\"num_mines\", [2] * len(completions))[idx]\n",
    "            \n",
    "            # Reconstruct game with correct dimensions\n",
    "            game = MinesweeperGame(rows=r, cols=c, num_mines=m, seed=seed)\n",
    "            for prev_action in move_history:\n",
    "                game.do_action(prev_action)\n",
    "\n",
    "            row, col = action.get(\"row\", -1), action.get(\"col\", -1)\n",
    "            action_type = action.get(\"type\", \"\")\n",
    "\n",
    "            # Criterion 7: Efficiency penalty applied to all valid moves\n",
    "            score = -0.5 \n",
    "            # Criterion 10: Out of bounds\n",
    "            if not (0 <= row < game.rows and 0 <= col < game.cols):\n",
    "                scores.append(score - 15.0)\n",
    "                continue\n",
    "\n",
    "            # State Checks\n",
    "            is_mine = (game._board[row][col] == -1)\n",
    "            is_revealed = ((row, col) in game._revealed)\n",
    "            is_flagged = ((row, col) in game._flagged)\n",
    "\n",
    "            if action_type == \"flag\":\n",
    "                if is_flagged:\n",
    "                    score += -12.0  # Criterion 8\n",
    "                elif is_revealed:\n",
    "                    score += -8.0   # Criterion 15\n",
    "                elif len(game._flagged) >= game.num_mines:\n",
    "                    score += -10.0  # Criterion 11\n",
    "                else:\n",
    "                    if is_mine:\n",
    "                        score += 20.0  # Criterion 1\n",
    "                        \n",
    "                        # Check for Win Condition (Criterion 13)\n",
    "                        if len(game._flagged) + 1 == game.num_mines and len(game._revealed) == (game.rows * game.cols - game.num_mines):\n",
    "                            score += 100.0\n",
    "                    else:\n",
    "                        score += -15.0 # Criterion 2\n",
    "\n",
    "            elif action_type == \"reveal\":\n",
    "                if is_revealed:\n",
    "                    score += -12.0  # Criterion 9\n",
    "                elif is_flagged:\n",
    "                    score += -8.0   # Criterion 14\n",
    "                elif is_mine:\n",
    "                    score += -30.0  # Criterion 3\n",
    "                else:\n",
    "                    # Adjacency Heuristic\n",
    "                    is_adjacent_to_revealed = False\n",
    "                    for dr in [-1, 0, 1]:\n",
    "                        for dc in [-1, 0, 1]:\n",
    "                            nr, nc = row + dr, col + dc\n",
    "                            if 0 <= nr < game.rows and 0 <= nc < game.cols:\n",
    "                                if (nr, nc) in game._revealed:\n",
    "                                    is_adjacent_to_revealed = True\n",
    "                                    break\n",
    "                        if is_adjacent_to_revealed: break\n",
    "                    \n",
    "                    # Criterion 4 & 5: Adjacency Logic vs YOLO Guessing\n",
    "                    if is_adjacent_to_revealed:\n",
    "                        score += 20.0  # Logical deducible move\n",
    "                    else:\n",
    "                        if len(game._revealed) == 0:\n",
    "                            score += 10.0  # Completely fine if it's the very first move of the game\n",
    "                        else:\n",
    "                            score += -5.0  # YOLO penalty: Guessing in the dark mid-game is bad\n",
    "\n",
    "                    # Criterion 6: Zero-Cell Discovery Bonus\n",
    "                    if game._board[row][col] == 0:\n",
    "                        score += 10.0 \n",
    "\n",
    "                    # Check for Win Condition (Criterion 13)\n",
    "                    if len(game._revealed) + 1 == (game.rows * game.cols - game.num_mines):\n",
    "                        score += 100.0\n",
    "            else:\n",
    "                score += -10.0 # Unknown action type\n",
    "\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            scores.append(0.0) # Fallback\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Dataset|\n",
    "\n",
    "Generate diverse game states for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training dataset...\n",
      "Created EXACTLY 1000 training examples (all ongoing games)\n",
      "  Fresh games: 279 (27.9%)\n",
      "  Mid-game states: 721 (72.1%)\n",
      "\n",
      "Example training prompt:\n",
      "You are a Minesweeper AI. Analyze the state and make a move.\n",
      "\n",
      "CRITICAL RULES:\n",
      "- DO NOT use <think> tags.\n",
      "- DO NOT output any reasoning, explanations, or text.\n",
      "- Start your response IMMEDIATELY with { and end with }.\n",
      "- ONLY output a valid JSON object.\n",
      "\n",
      "Game state:\n",
      "{\n",
      "  \"board_grid\": \"0 0 0 1 . .\\n1 1 1 1 . .\\n. . . . 1 .\\n. . . . . .\\n. . . . . .\\n. 1 . . . .\",\n",
      "  \"rows\": 6,\n",
      "  \"cols\": 6,\n",
      "  \"mines\": 5...\n",
      "Seed: 15795, Previous moves: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "def generate_game_states(num_samples=1000, rows=6, cols=6, num_mines=5,\n",
    "                         rng_seed=42):\n",
    "    \"\"\"\n",
    "    Generate EXACTLY num_samples diverse Minesweeper game states.\n",
    "    \n",
    "    Mix of:\n",
    "    - Fresh games (20-30%)\n",
    "    - Mid-game states (70-80%)\n",
    "    \n",
    "    IMPORTANTLY: Stores seed + move_history (as JSON string) so reward\n",
    "    function can reconstruct the EXACT game state!\n",
    "    \n",
    "    Keeps generating until we have exactly num_samples valid ongoing games.\n",
    "    \n",
    "    Args:\n",
    "        rng_seed: Seed for numpy/random RNG for reproducibility.\n",
    "    \"\"\"\n",
    "    # Seed RNG for reproducibility across runs\n",
    "    np.random.seed(rng_seed)\n",
    "    random.seed(rng_seed)\n",
    "\n",
    "    dataset_items = []\n",
    "    attempts = 0\n",
    "    max_attempts = num_samples * 3  # Safety limit\n",
    "    \n",
    "    while len(dataset_items) < num_samples and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        seed = np.random.randint(100000)\n",
    "        game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n",
    "        \n",
    "        # Make 0-5 random moves (0 = fresh game, 1-5 = mid-game)\n",
    "        num_moves = np.random.randint(0, 6)\n",
    "        move_history = []\n",
    "        \n",
    "        for _ in range(num_moves):\n",
    "            board = game.get_visible_board()\n",
    "            unrevealed = []\n",
    "            for r in range(rows):\n",
    "                for c in range(cols):\n",
    "                    if board[r][c] == '.':\n",
    "                        unrevealed.append((r, c))\n",
    "            \n",
    "            if unrevealed and game.state() == \"ongoing\":\n",
    "                r, c = random.choice(unrevealed)\n",
    "                action = {\"type\": \"reveal\", \"row\": r, \"col\": c}\n",
    "                game.do_action(action)\n",
    "                move_history.append(action)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Only add ongoing games (skip failed/completed games)\n",
    "        if game.state() == \"ongoing\":\n",
    "            prompt_text = format_state_for_llm(game)\n",
    "            dataset_items.append({\n",
    "                \"prompt\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                \"seed\": seed,  # Store seed to reconstruct game\n",
    "                # IMPORTANT: Serialize as JSON string to avoid HF Dataset\n",
    "                # schema inference mangling list-of-dicts into dict-of-lists\n",
    "                \"move_history\": json.dumps(move_history),\n",
    "            })\n",
    "    \n",
    "    return Dataset.from_list(dataset_items)\n",
    "\n",
    "# Generate training dataset\n",
    "print(\"Generating training dataset...\")\n",
    "dataset = generate_game_states(num_samples=1000, rows=6, cols=6, num_mines=5)\n",
    "print(f\"Created EXACTLY {len(dataset)} training examples (all ongoing games)\")\n",
    "\n",
    "# Count fresh vs mid-game\n",
    "fresh_count = sum(1 for item in dataset if item[\"move_history\"] == \"[]\")\n",
    "print(f\"  Fresh games: {fresh_count} ({fresh_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Mid-game states: {len(dataset) - fresh_count} ({(len(dataset)-fresh_count)/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample training prompt:\")\n",
    "print(dataset[0][\"prompt\"][0][\"content\"][:400] + \"...\")\n",
    "print(f\"Seed: {dataset[0]['seed']}, Previous moves: {len(json.loads(dataset[0]['move_history']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure GRPO Training\n",
    "\n",
    "Set up GRPO trainer with all hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n",
      "Training configuration:\n",
      "  Max steps: 20\n",
      "  Generations per state: 4\n",
      "  Learning rate: 5e-05\n",
      "  LoRA rank: 64\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Calculate max lengths\n",
    "max_prompt_length = 600   # JSON state prompt\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "# GRPO Configuration\n",
    "training_args = GRPOConfig(\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,  # Issue #6: was 1; 4 gives 16 effective completions per update\n",
    "    num_generations = 4,  # Generate 4 actions per state\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    max_steps = 20,      # Adjust based on compute budget\n",
    "    save_steps = 5,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"minesweeper_custom_outputs\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Generations per state: {training_args.num_generations}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  LoRA rank: {lora_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval callback created: plays 5 games every 50 steps\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MinesweeperEvalCallback(TrainerCallback):\n",
    "    \"\"\"Periodically play games during training and log win rate.\n",
    "    (Issue #8: no validation / reward tracking in the original notebook.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_every_steps=50, num_games=5):\n",
    "        self.eval_every_steps = eval_every_steps\n",
    "        self.num_games = num_games\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, processing_class=None, **kwargs):\n",
    "        if state.global_step % self.eval_every_steps != 0:\n",
    "            return\n",
    "\n",
    "        tokenizer = processing_class\n",
    "        if tokenizer is None or model is None:\n",
    "            return\n",
    "\n",
    "        # Temporarily set model to eval mode\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        wins = 0\n",
    "        for i in range(self.num_games):\n",
    "            game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=10000 + i)\n",
    "            moves = 0\n",
    "            while game.state() == \"ongoing\" and moves < 50:\n",
    "                prompt = format_state_for_llm(game)\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "                output = model.generate(\n",
    "                    **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "                    temperature=0.7,\n",
    "                    max_new_tokens=128,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "                response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                action = parse_llm_action(response)\n",
    "                if action is None:\n",
    "                    break\n",
    "                game.do_action(action)\n",
    "                moves += 1\n",
    "            if game.state() == \"success\":\n",
    "                wins += 1\n",
    "\n",
    "        win_rate = wins / self.num_games\n",
    "        print(f\"\\n[Eval @ step {state.global_step}] Win rate: {wins}/{self.num_games} ({win_rate*100:.0f}%)\\n\")\n",
    "\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "eval_callback = MinesweeperEvalCallback(eval_every_steps=50, num_games=5)\n",
    "print(\"Eval callback created: plays 5 games every 50 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "Start GRPO training with reward functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576 of 4,154,588,672 (3.18% trained)\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 40960, 'temperature': 0.6, 'top_p': 0.95}. If this is not desired, please set these values explicitly.\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2560], which does not match the required output shape [16, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 04:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / valid_json_reward / mean</th>\n",
       "      <th>rewards / valid_json_reward / std</th>\n",
       "      <th>rewards / gameplay_scores / mean</th>\n",
       "      <th>rewards / gameplay_scores / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-102.843750</td>\n",
       "      <td>14.312500</td>\n",
       "      <td>423.750000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>-56.562500</td>\n",
       "      <td>13.750001</td>\n",
       "      <td>-46.281250</td>\n",
       "      <td>14.875001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.024335</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-102.843750</td>\n",
       "      <td>14.312500</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.061265</td>\n",
       "      <td>-56.562500</td>\n",
       "      <td>13.750001</td>\n",
       "      <td>-46.281250</td>\n",
       "      <td>14.875001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-102.843750</td>\n",
       "      <td>14.312500</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.087364</td>\n",
       "      <td>-56.562500</td>\n",
       "      <td>13.750001</td>\n",
       "      <td>-46.281250</td>\n",
       "      <td>14.875001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.035295</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.080578</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-102.843750</td>\n",
       "      <td>14.312500</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>-56.562500</td>\n",
       "      <td>13.750001</td>\n",
       "      <td>-46.281250</td>\n",
       "      <td>14.875001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-102.218750</td>\n",
       "      <td>15.562500</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.105839</td>\n",
       "      <td>-56.562500</td>\n",
       "      <td>13.750001</td>\n",
       "      <td>-45.656250</td>\n",
       "      <td>17.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-94.437500</td>\n",
       "      <td>17.970028</td>\n",
       "      <td>422.687500</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.098134</td>\n",
       "      <td>-53.125000</td>\n",
       "      <td>18.786077</td>\n",
       "      <td>-41.312500</td>\n",
       "      <td>23.738770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-95.062500</td>\n",
       "      <td>17.278509</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.151050</td>\n",
       "      <td>-53.125000</td>\n",
       "      <td>18.786077</td>\n",
       "      <td>-41.937500</td>\n",
       "      <td>22.106466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-102.843750</td>\n",
       "      <td>14.312500</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.112521</td>\n",
       "      <td>-56.562500</td>\n",
       "      <td>13.750001</td>\n",
       "      <td>-46.281250</td>\n",
       "      <td>14.875001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-94.437500</td>\n",
       "      <td>17.970028</td>\n",
       "      <td>422.875000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.109956</td>\n",
       "      <td>-53.125000</td>\n",
       "      <td>18.786077</td>\n",
       "      <td>-41.312500</td>\n",
       "      <td>23.738770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-63.312500</td>\n",
       "      <td>31.125000</td>\n",
       "      <td>415.687500</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>379.666687</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.147136</td>\n",
       "      <td>-39.375000</td>\n",
       "      <td>27.500002</td>\n",
       "      <td>-23.937500</td>\n",
       "      <td>34.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-95.687500</td>\n",
       "      <td>16.526651</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.130595</td>\n",
       "      <td>-53.125000</td>\n",
       "      <td>18.786077</td>\n",
       "      <td>-42.562500</td>\n",
       "      <td>20.323120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-65.187500</td>\n",
       "      <td>47.845028</td>\n",
       "      <td>405.187500</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>363.800018</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.222264</td>\n",
       "      <td>-39.375000</td>\n",
       "      <td>27.500002</td>\n",
       "      <td>-25.812500</td>\n",
       "      <td>32.404667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>-88.531250</td>\n",
       "      <td>14.312500</td>\n",
       "      <td>413.687500</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.124402</td>\n",
       "      <td>-49.687500</td>\n",
       "      <td>22.171209</td>\n",
       "      <td>-38.843750</td>\n",
       "      <td>23.985218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2560], which does not match the required output shape [16, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2560], which does not match the required output shape [16, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2560], which does not match the required output shape [16, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n",
      "/usr/local/lib/python3.12/dist-packages/unsloth/kernels/utils.py:743: UserWarning: An output with one or more elements was resized since it had shape [1, 16, 2560], which does not match the required output shape [16, 1, 2560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /app/pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  out = torch_matmul(X, W.t(), out = out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=0.0003303109772332391, metrics={'train_runtime': 282.3787, 'train_samples_per_second': 1.133, 'train_steps_per_second': 0.071, 'total_flos': 0.0, 'train_loss': 0.0003303109772332391})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        valid_json_reward,   # Reward valid JSON format\n",
    "        gameplay_scores,     # Reward good gameplay\n",
    "    ],\n",
    "    args = training_args,\n",
    "    use_reference_model = False,\n",
    "    train_dataset = dataset,\n",
    "    callbacks = [eval_callback],  # Periodic gameplay evaluation\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Trained Model\n",
    "\n",
    "Evaluate the finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trained Model Response ===\n",
      "<think>\n",
      "Okay, let's see. The user provided a Minesweeper game state where the board is completely unrevealed, there are 5 mines, and no flags or cells revealed yet. My job is to figure out the best move here.\n",
      "\n",
      "First, since the board is all unrevealed, the AI needs to decide whether to flag a cell or reveal one. But since no flags are placed yet, maybe starting by revealing a cell that's likely to be safe. But with 5 mines on a 6x6 grid, there are 36 cells total. So 5 mines mean 31 safe cells\n",
      "\n",
      "Parsed action: None\n"
     ]
    }
   ],
   "source": [
    "# Test on new game\n",
    "test_game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "test_prompt = format_state_for_llm(test_game)\n",
    "\n",
    "# Removed reasoning_effort=\"low\" for train/eval consistency\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": test_prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    ")\n",
    "\n",
    "print(\"=== Trained Model Response ===\")\n",
    "output = model.generate(\n",
    "    **tokenizer(test_text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.7,\n",
    "    max_new_tokens = 128,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n",
    "\n",
    "# Parse and test action\n",
    "response_text = tokenizer.decode(output[0])\n",
    "action = parse_llm_action(response_text)\n",
    "print(f\"\\nParsed action: {action}\")\n",
    "\n",
    "if action:\n",
    "    test_game.do_action(action)\n",
    "    print(f\"\\nGame state after action: {test_game.state()}\")\n",
    "    print(test_game.pretty_print())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Play Complete Games\n",
    "\n",
    "Test the model on multiple complete games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "RAW MODEL OUTPUT:\n",
      "{\"type\": \"flag\", \"row\": 3, \"col\": 3}\n",
      "PARSED ACTION: {'type': 'flag', 'row': 3, 'col': 3}\n",
      "\n",
      "FINAL GAME STATE: ongoing\n",
      "TOTAL MOVES: 50\n"
     ]
    }
   ],
   "source": [
    "def play_full_game(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    rows=4,\n",
    "    cols=4,\n",
    "    num_mines=2,\n",
    "    seed=None,\n",
    "    max_moves=50\n",
    "):\n",
    "    \"\"\"Play a complete Minesweeper game with the model\"\"\"\n",
    "    game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n",
    "    moves = 0\n",
    "\n",
    "    while game.state() == \"ongoing\" and moves < max_moves:\n",
    "        # ðŸ”’ Force safe first move\n",
    "        if moves == 0:\n",
    "            action = {\"type\": \"reveal\", \"row\": 0, \"col\": 0}\n",
    "            game.do_action(action)\n",
    "            moves += 1\n",
    "            continue\n",
    "\n",
    "        # Format game state\n",
    "        prompt = format_state_for_llm(game)\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You MUST output ONLY a valid JSON object. No explanation. No <think>.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Decode ONLY generated tokens\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "        response = tokenizer.decode(\n",
    "            output[0][input_len:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        print(\"\\nRAW MODEL OUTPUT:\")\n",
    "        print(response)\n",
    "\n",
    "        action = parse_llm_action(response)\n",
    "        print(\"PARSED ACTION:\", action)\n",
    "\n",
    "        if action is None:\n",
    "            print(\"âŒ ACTION IS NONE â€” STOPPING GAME\")\n",
    "            break\n",
    "\n",
    "        game.do_action(action)\n",
    "        moves += 1\n",
    "\n",
    "    return game, moves\n",
    "\n",
    "\n",
    "# =======================\n",
    "# SINGLE-GAME DEBUG RUN\n",
    "# =======================\n",
    "\n",
    "game, moves = play_full_game(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    rows=4,\n",
    "    cols=4,\n",
    "    num_mines=2,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "print(\"\\nFINAL GAME STATE:\", game.state())\n",
    "print(\"TOTAL MOVES:\", moves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model\n",
    "\n",
    "Save your trained model for competition submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: my_minesweeper_model/\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"my_minesweeper_model\")\n",
    "tokenizer.save_pretrained(\"my_minesweeper_model\")\n",
    "\n",
    "print(\"Model saved to: my_minesweeper_model/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Tips\n",
    "\n",
    "## Improve Your Model:\n",
    "\n",
    "1. **Adjust Reward Functions**\n",
    "   - Increase rewards for logical deduction\n",
    "   - Add penalties for random moves\n",
    "   - Reward flagging correct mines\n",
    "\n",
    "2. **Tune Hyperparameters**\n",
    "   - Increase `max_steps` for longer training\n",
    "   - Adjust `learning_rate` (try 1e-5 to 1e-4)\n",
    "   - Increase `lora_rank` for more capacity\n",
    "   - Adjust `num_generations` (2-8)\n",
    "\n",
    "3. **Better Training Data**\n",
    "   - Generate more diverse states\n",
    "   - Include harder scenarios (more mines)\n",
    "   - Add states requiring logical deduction\n",
    "\n",
    "4. **Advanced Techniques**\n",
    "   - Multi-step rollouts in reward function\n",
    "   - Curriculum learning (easy â†’ hard boards)\n",
    "   - Ensemble multiple models\n",
    "\n",
    "## Team Strategy:\n",
    "- Experiment with different reward functions\n",
    "- Try different board sizes during training\n",
    "- Analyze failed games to improve rewards\n",
    "- Use temperature sampling during evaluation\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
